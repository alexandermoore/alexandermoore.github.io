<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-03-19T13:21:40-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Alex’s Website</title><subtitle>My personal website where I write about various machine learning related things :)</subtitle><entry><title type="html">MLOps Cheat Sheet</title><link href="http://localhost:4000/posts/cheat-sheet-mlops" rel="alternate" type="text/html" title="MLOps Cheat Sheet" /><published>2023-03-19T00:00:00-04:00</published><updated>2023-03-19T00:00:00-04:00</updated><id>http://localhost:4000/posts/cheat-sheet-mlops</id><content type="html" xml:base="http://localhost:4000/posts/cheat-sheet-mlops">&lt;p&gt;This is a living document, and I’m no expert :). I’m just documenting things I learn about MLOps here. I’ll include any resources I’ve found helpful at the end.&lt;/p&gt;

&lt;h2 id=&quot;data-drift&quot;&gt;Data Drift&lt;/h2&gt;
&lt;p&gt;Data drift occurs when the distribution of data in production changes over time. For example, a unimodal feature may become bimodal, or a power-law distributed feature may gain a thicker tail. To give a concrete example, a price feature distribution may change after the release of new and popular expensive items on an eCommerce website.&lt;/p&gt;

&lt;h3 id=&quot;why-is-data-drift-an-issue&quot;&gt;Why is data drift an issue?&lt;/h3&gt;
&lt;p&gt;Data drift can negatively impact model performance. A shift in the feature distribution means the input values no longer match what the model was trained on. Perhaps the model never saw very high prices during training, so its predictions on these values would be unpredictable.&lt;/p&gt;

&lt;h3 id=&quot;how-to-monitor-for-data-drift&quot;&gt;How to monitor for data drift?&lt;/h3&gt;
&lt;p&gt;This is often done by tracking feature distributions (and missing value counts!). Here are some ways:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Statistical Properties&lt;/strong&gt;: Compute statistics about the data, and compare them to those present at training time. Some statistics include:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;mean&lt;/strong&gt;: A simple measure, but it does not capture many types of drift (ex. unimodal -&amp;gt; bimodal might be missed!)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;variance&lt;/strong&gt;: Another simple measure.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;distributional distance measure&lt;/strong&gt;: Use a distance measure to compare the current distribution to the old one. A good general purpose one is &lt;a href=&quot;https://en.wikipedia.org/wiki/Earth_mover%27s_distance&quot;&gt;Earth Mover’s Distance&lt;/a&gt; (&lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wasserstein_distance.html&quot;&gt;scikit implementation&lt;/a&gt;). Here’s a &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_distance&quot;&gt;list of other measures&lt;/a&gt;.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;percentiles&lt;/strong&gt; Compute percentiles on the prod distribution at the time the model was trained, and check whether the current percentiles are similar or different. If you want confidence intervals, you can use bootstrap, jack knife (sub-method of cross validation) or another &lt;a href=&quot;https://en.wikipedia.org/wiki/Resampling_(statistics)&quot;&gt;resampling method&lt;/a&gt; to establish confidence intervals around these percentiles. Then, in production, check if the production percentiles fall within this interval.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Visual tracking&lt;/strong&gt;: Compute feature histograms with some time frequency (ex. daily) and surface them in some sort of dashboard. Allow the user to compare current histograms to historical ones.
    &lt;ul&gt;
      &lt;li&gt;I would recommend this as a bonus addition on top of the other techniques.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-fix-data-drift&quot;&gt;How to fix data drift?&lt;/h3&gt;
&lt;p&gt;A common way is to include samples from the new distribution as part of model training. This could be a model retrain from scratch, or some sort of online learning update.&lt;/p&gt;

&lt;p&gt;If data drift is detected, a model retrain could be kicked off automatically, or an alert could be sent to the team to investigate.&lt;/p&gt;

&lt;h2 id=&quot;concept-drift&quot;&gt;Concept Drift&lt;/h2&gt;
&lt;p&gt;Concept drift is when the relationship between the inputs and outputs changes. This means the model isn’t modeling the right mapping of $X -&amp;gt; y$ anymore. For example, users may not want to buy a particular category of items anymore because a super famous YouTuber said it’s bad, so maybe features that originally correlated with purchases in that category no longer do.&lt;/p&gt;

&lt;h3 id=&quot;why-is-concept-drift-an-issue&quot;&gt;Why is concept drift an issue?&lt;/h3&gt;
&lt;p&gt;Since the model is no longer modeling the correct relationship between inputs and outputs, its performance will degrade. It’s like your model was born in 1855, and suddenly the year is 2023. If you ask it the fastest travel time from New York to London, it’s not going to give the right answer.&lt;/p&gt;

&lt;h3 id=&quot;how-to-monitor-for-concept-drift&quot;&gt;How to monitor for concept drift?&lt;/h3&gt;
&lt;p&gt;Since concept drift reflects an incorrect model, it can be measured using evaluation metrics.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Note that data drift can also cause metrics to go down, so having the data drift monitoring in place could help differentiate the two!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are a couple ways of dealing with concept drift:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Metric monitoring&lt;/strong&gt;: Monitor metrics such as AUC, F1, etc and see if they have been decreasing over time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prediction confidence monitoring&lt;/strong&gt;: For classification, prediction scores measure confidence. We can monitor the average score (say, on the last day of data) and see if it has been changing over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-fix-concept-drift&quot;&gt;How to fix concept drift?&lt;/h3&gt;
&lt;p&gt;Similar to data drift, we can handle concept drift by including data which demonstrates the new $X -&amp;gt; y$ relationship in our model. This usually means retraining the model on recent data, or updating it with some sort of online learning.&lt;/p&gt;

&lt;h2 id=&quot;new-model-ship-decision&quot;&gt;New Model Ship Decision&lt;/h2&gt;
&lt;p&gt;When a new model is trained, it may or may not outperform the existing one. Here are a few ways to decide whether or not to ship, and how to do it:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Offline evaluation on recent data&lt;/strong&gt;: Set aside recent production data and evaluate both the existing and new models on it. See if the new model performs better than the existing one.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Shadow mode&lt;/strong&gt;: Run the new model in shadow mode on production traffic. Compare the accuracy results and other important performance metrics (ex. latency) from both models over a duration. This is recommended for very important models, since it gives you a fuller picture.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Slow Rollout&lt;/strong&gt;: Roll out the new model on a small amount of traffic. If performance looks good, try increasing it bit by bit.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;retrain-frequency&quot;&gt;Retrain Frequency&lt;/h2&gt;
&lt;p&gt;Given data and concept drift are common, model retraining is often necessary. Here are some ways to decide how often to retrain the model:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dynamic based on production performance&lt;/strong&gt;: If some model metric drops below a threshold, trigger a retrain. A retrain could also be triggered once data drift has been detected.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Based on offline performance&lt;/strong&gt;: We can do this at the time we’re training the model. The model could be trained on older data and evaluated on data up to $N$ days after that. If performance starts to degrade after $k&amp;lt;N$ days, the model could be retrained every $k$ days.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The &lt;em&gt;Adventures in Machine Learning&lt;/em&gt; podcast has helpful episodes with common problems and various solutions (I was inspired to write this post after listening!)
    &lt;ul&gt;
      &lt;li&gt;Spotify links to episodes I listened to on a nice walk: &lt;a href=&quot;https://open.spotify.com/episode/4uYadREg9IIQUl409bP1Z3?si=wsNs4LIZS2OFrnXIoFemxw&amp;amp;nd=1&quot;&gt;Part 1&lt;/a&gt;, &lt;a href=&quot;https://open.spotify.com/episode/7Fwg0v3HnwCnJYQb13Rn9R?si=E1bdnX_FRReTQNnKcDw0Vg&quot;&gt;Part 2&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://neptune.ai/blog/category/mlops&quot;&gt;Neptune.ai’s blog&lt;/a&gt; has some helpful posts. Here’s one on &lt;a href=&quot;https://neptune.ai/blog/concept-drift-best-practices&quot;&gt;data vs. concept drift&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.shreya-shankar.com/&quot;&gt;Shreya Shankar’s blog&lt;/a&gt; has some useful MLOps posts as well.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="_drafts" /><category term="notebook" /><summary type="html">This is a living document, and I’m no expert :). I’m just documenting things I learn about MLOps here. I’ll include any resources I’ve found helpful at the end.</summary></entry><entry><title type="html">ROC AUC vs. PR AUC for Class Imbalance</title><link href="http://localhost:4000/posts/roc-vs-pr-auc" rel="alternate" type="text/html" title="ROC AUC vs. PR AUC for Class Imbalance" /><published>2023-03-18T00:00:00-04:00</published><updated>2023-03-18T00:00:00-04:00</updated><id>http://localhost:4000/posts/roc-vs-pr-auc</id><content type="html" xml:base="http://localhost:4000/posts/roc-vs-pr-auc">&lt;p&gt;I was just thinking about ROC AUC and PR AUC. Both are binary classification metrics, but ROC AUC can suffer in the presence of severe class imbalance. Here’s a way to see how PR AUC might do better.&lt;/p&gt;

&lt;p&gt;By the end of this post we’ll see why ROC AUC can give you great results even for a bad model in the presence of class imbalance, and how PR AUC’s focus on the positive class helps avoid this pitfall.&lt;/p&gt;

&lt;h2 id=&quot;roc-auc-basics&quot;&gt;ROC AUC Basics&lt;/h2&gt;

&lt;p&gt;We’ll start by examining ROC AUC. Feel free to skip this section if you are already familiar. The ROC curve depends on 2 things:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;True positive rate (aka recall) (Y axis):&lt;/strong&gt; Among all the positives, what fraction did the model say were positive?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ \frac{\text{num positives classified positive}}{\text{total num positive}} $$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;False positive rate (X axis):&lt;/strong&gt; Among all the negatives, what fraction did the model say were positive?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ \frac{\text{num negatives classified positive}}{\text{total num negative}} $$&lt;/p&gt;

&lt;p&gt;Here’s an example of a typical ROC curve:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebook_files/drafts/ROC-Vs-PR-AUC/output_3_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each point on the line corresponds to a particular model score threshold, whereby all examples above the threshold are classified as positive and all the examples below are classified as negative. We then compute the &lt;strong&gt;recall&lt;/strong&gt; and &lt;strong&gt;false positive rate&lt;/strong&gt; given that threshold to get a point. The above chart was created from 50 different thresholds between 0 and 1, as you can see here:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebook_files/drafts/ROC-Vs-PR-AUC/output_5_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The area underneath this curve is called the ROC AUC, and it is a common way of measuring model performance. A value of 1 is perfect and 0 is awful. In the above diagram, the area is &lt;strong&gt;0.82&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;understanding-the-curve&quot;&gt;Understanding the curve&lt;/h3&gt;
&lt;p&gt;It turns out that the bottom leftmost point corresponds to a threshold of 1. At this threshold, recall is 0 because no positive examples have scores above the threshold, and the false positive rate is also 0 since no negatives were classified as positive.&lt;/p&gt;

&lt;p&gt;As we decrease the threshold and allow more examples to be classified as positive, depending on whether they have positive or negative labels, recall or the false positive rate will increase. If we decrease our threshold and more positive examples are correctly classified as positive, recall will go up. Conversely, if we decrease our threshold and more negative examples are misclassified as positive, the false positive rate will go up. Overall this moves us up and to the right along the curve (recall or false positive rate is always increasing as we lower the threshold), resulting in the shape we see above.&lt;/p&gt;

&lt;h3 id=&quot;concrete-example-perfect-and-worst-case-models&quot;&gt;Concrete Example: Perfect and Worst Case Models&lt;/h3&gt;
&lt;p&gt;Suppose you have a model that scores 100 positives above 1000 negatives with the following scores:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;100 positive examples with scores over 0.80&lt;/li&gt;
  &lt;li&gt;1000 negative examples with scores below 0.80&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a perfect model as far as ordering is concerned, so let’s see if the ROC AUC is 1.&lt;/p&gt;

&lt;p&gt;To see this, we’ll start with a super high threshold of &lt;strong&gt;1&lt;/strong&gt;, and slowly decrease it. As we decrease it, more and more instances will be labeled as positive by the model, and we can observe what happens to the ROC AUC curve.&lt;/p&gt;

&lt;p&gt;As we decrease the threshold, we start including more and more of those 100 positives. The recall slowly climbs from 0 to 1. Throughout this process, we aren’t classifying any negatives as positive. So the false positive rate stays at 0, and all these points on the curve have x coordinate 0.&lt;/p&gt;

&lt;p&gt;Once our threshold hits 0.80, we’ve included all the positives, and our recall reaches 1. As we lower the threshold further, we start to include some negatives. The false positive rate starts to increase. But for all these different false positive rates, the recall is still 1. So as we move along the x axis, the y coordinate is always 1. In the end, we end up with a horizontal line from (0,1) to (1,1). The area under this is 1, so the ROC AUC is 1. We can see this below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebook_files/drafts/ROC-Vs-PR-AUC/output_9_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similarly, consider a terrible model that scores the 1000 negative examples above the 100 positive ones:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1000 negative examples with scores over 0.80&lt;/li&gt;
  &lt;li&gt;100 positive examples with scores below 0.80&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Its ROC AUC curve would look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebook_files/drafts/ROC-Vs-PR-AUC/output_11_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We again start with a high threshold of &lt;strong&gt;1&lt;/strong&gt; and slowly decrease it. As we decrease the threshold, we start misclassifying those 1000 negatives as positive, which increases the false positive rate. During this time we aren’t correctly classifying any of the 100 positives yet, so all these points have y coordinate (recall) 0. By the time our threshold reaches 0.80, we have classified all 1000 negatives as positive. This means our false positive rate is 1. Dropping the threshold further starts to include more positives and recall goes up, but we don’t get any area from it since it’s a vertcal line from (1, 0) to (1, 1).&lt;/p&gt;

&lt;p&gt;Thus the ROC AUC is 0.&lt;/p&gt;

&lt;h2 id=&quot;introducing-class-imbalance&quot;&gt;Introducing Class Imbalance&lt;/h2&gt;
&lt;p&gt;Now that we understand ROC AUC, what happens to this metric when class imbalance is introduced? What if there are a lot more negatives than positives in our data?&lt;/p&gt;

&lt;p&gt;We’ll start with a balanced case. Suppose our model scores examples as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;100 positives with scores above 0.90&lt;/li&gt;
  &lt;li&gt;350 negatives with scores between 0.90 and 0.80&lt;/li&gt;
  &lt;li&gt;200 positives with scores between 0.50 and 0.80&lt;/li&gt;
  &lt;li&gt;250 more negatives with scores below 0.50&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This model could be better. It’s scoring those 350 negatives a bit high.&lt;/p&gt;

&lt;p&gt;If we do the same threshold lowering experiment, we’ll first include these 100 positives as our threshold lowers to 0.90. At this point recall will be $\frac{100}{300}$ and the false positive rate will be 0. Continuing on, we’ll include those 350 negatives as our threshold lowers to 0.80. This whole time we’ll be moving along the X axis as our false positive rate increases, but the recall will remain the same. By the time the threshold reaches 0.80, the false positive rate is $\frac{350}{600}$.&lt;/p&gt;

&lt;p&gt;Let’s keep lowering the threshold from 0.80 to 0.50. As we do this we include the 200 positives. The false positive rate stays at $\frac{350}{600}$ but recall increases. Once we hit a threshold of 0.50 we have included all 300 positives, and recall reaches 1. As we further lower the threshold, the false positive rate starts to increase again since we’re including those last 250 negatives, but recall is 1 the whole time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebook_files/drafts/ROC-Vs-PR-AUC/output_14_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The ROC AUC here is &lt;strong&gt;0.61&lt;/strong&gt;. So not great, as we would hope.&lt;/p&gt;

&lt;p&gt;Now let’s make it much more imbalanced. Suppose our model scores examples as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;100 positives with scores above 0.90&lt;/li&gt;
  &lt;li&gt;350 negatives with scores between 0.90 and 0.80&lt;/li&gt;
  &lt;li&gt;200 positives with scores between 0.50 and 0.80&lt;/li&gt;
  &lt;li&gt;10,650 more negatives with scores below 0.50&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ll do the same threshold decreasing experiment, starting from 1. When our threshold reaches 0.90, we’ve included those 100 positives as before, so recall is again $\frac{100}{300}$ and the false positive rate is 0. Once our threshold hits 0.80, recall is still the same and the false positive rate is $\frac{100}{11,000}$. Then we start including those 200 positives, and the recall shoots up to 1. Finally, we begin including the 10,900 negatives, which brings our false positive rate to 1.&lt;/p&gt;

&lt;p&gt;Let’s see what the curve looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebook_files/drafts/ROC-Vs-PR-AUC/output_16_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The ROC AUC here is… &lt;strong&gt;0.98&lt;/strong&gt;?!&lt;/p&gt;

&lt;p&gt;That’s a great score! But wait, isn’t this model not good? Imagine if we used it in production and it behaved like this. Many of the examples with high scores in the 0.80 - 0.90 range are negatives, and most of the positives have scores below 0.80. So ROC AUC has deceived us here.&lt;/p&gt;

&lt;p&gt;How can we address this situation? Enter PR AUC.&lt;/p&gt;

&lt;h2 id=&quot;pr-auc-to-the-rescue&quot;&gt;PR AUC to the rescue&lt;/h2&gt;
&lt;p&gt;PR AUC is the area under the precision recall curve. Its axes are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Precision (y-axis):&lt;/strong&gt; Of the instances that were classified as positive, what fraction are actually positive?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ \frac{\text{num classified positive and actually positive}}{\text{num classified positive}}$$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Recall (x-axis):&lt;/strong&gt; Of all the positive instances, what fraction did the model classify as positive?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ \frac{\text{num positives classified positive}}{\text{total num positive}}$$&lt;/p&gt;

&lt;p&gt;A typical PR curve might look like this (it doesn’t have to though):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebook_files/drafts/ROC-Vs-PR-AUC/output_18_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(By the way, the PR AUC of that curve is &lt;strong&gt;0.79&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;Let’s revisit our class imbalance situation. Again suppose our model scores examples as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;100 positives with scores above 0.90&lt;/li&gt;
  &lt;li&gt;350 negatives with scores between 0.90 and 0.80&lt;/li&gt;
  &lt;li&gt;200 positives with scores between 0.50 and 0.80&lt;/li&gt;
  &lt;li&gt;250 more negatives with scores below 0.50&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will again start with a high threshold and decrease it. Once the threshold decreases to 0.90, those 100 positives are classified as positive, so precision is 1 and recall is $\frac{100}{300}$.&lt;/p&gt;

&lt;p&gt;As our threshold decreases to 0.80 we classify those 350 negatives as positive. Recall remains the same, but precision drops to $\frac{100}{450}$.&lt;/p&gt;

&lt;p&gt;Once the threshold goes down to 0.50, we’ll have included those 200 positives. Recall will increase to 1, and precision will increase to $\frac{300}{650}$.&lt;/p&gt;

&lt;p&gt;After dropping our threshold below 0.50, we will begin including the 250 negatives. Recall will remain at 1, but precision will slowly drop. After the first of those 250 negatives precision will be $\frac{300}{651}$, then $\frac{300}{652}$, etc. all the way to $\frac{300}{900}$ when our threshold reaches 0.&lt;/p&gt;

&lt;p&gt;Here’s the chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebook_files/drafts/ROC-Vs-PR-AUC/output_20_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PR AUC here is a lowly &lt;strong&gt;0.28&lt;/strong&gt;. Note that PR AUC has really penalized this model for misclassifying those 350 negatives as positive!&lt;/p&gt;

&lt;p&gt;Now what if instead of 250 negatives at the end, we had 10,900 negatives like before? How would PR AUC change? Would it jump up like ROC AUC did?&lt;/p&gt;

&lt;p&gt;If we look closely, these 250 negatives didn’t really matter. Since recall was already at 1, we were just moving down the y axis on the chart as we included those 250 negatives and decreased precision.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;So it wouldn’t matter if we had 250 negatives at the end, or 1 negative, or 10,000. The chart would look exactly the same, and the PR AUC would be exactly the same. As long as the positives are above a threshold, PR AUC doesn’t care how many negatives are below.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;bottom-line&quot;&gt;Bottom Line&lt;/h2&gt;

&lt;h4 id=&quot;false-positive-rate-vs-precision&quot;&gt;False Positive Rate vs. Precision&lt;/h4&gt;
&lt;p&gt;In ROC AUC, the problem metric was false positive rate. Adding a bunch of negatives (increasing the denominator by a lot) makes changes in the numerator a lot less impactful. This means that there’s little difference between a model that misclassifies 10 of 10,000 negatives as positive and one that misclassifies 50 of 10,000. Both will have low false positive rates, which keeps the x-axis value on the ROC curve low.&lt;/p&gt;

&lt;p&gt;If those 10,000 negatives all have low scores, then the false positive rate will only increase in earnest once the threshold is low enough for the model to misclassify the bulk of those 10,000 examples as positive, and by that time the model has probably already correctly classified most of the positives. Because of this, the recall will reach 1 while the false positive rate is still pretty low, which leads to a large area under the curve.&lt;/p&gt;

&lt;p&gt;Precision, by contrast, doesn’t care how many negatives there are. All it cares about is whether the examples that are classified as positive are actually positive. In fact, for the entire PR AUC chart, negatives only matter if they are misclassified as positive. Neither recall nor precision cares how many negatives the model correctly classifies as negative, as long as they all have lower scores than the positives.&lt;/p&gt;

&lt;h4 id=&quot;final-summary&quot;&gt;Final Summary&lt;/h4&gt;
&lt;p&gt;So as we can see, ROC AUC and PR AUC measure different things:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;PR AUC focuses almost entirely on the positive class, only considering negatives insofar as they interfere with positive classifications. The model doesn’t get any brownie points for correctly classifying a huge set of negatives correctly as long as the positives are taken care of. In our imbalanced example, the model did a poor job by misclassifying those 350 negatives, so PR AUC gave it a heavy penalty.&lt;/li&gt;
  &lt;li&gt;ROC AUC considers both classes, and will give a model brownie points for correctly classifying lots of negatives and positives. In our imbalanced example, adding lots of negative examples with low scores caused ROC AUC to give the model lots of brownie points for these correct classifications, overlooking those 350 misclassified negatives.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Depending on the situation, it’s probably worth adding both to your metric suite!&lt;/p&gt;</content><author><name></name></author><category term="_drafts" /><category term="notebook" /><summary type="html">I was just thinking about ROC AUC and PR AUC. Both are binary classification metrics, but ROC AUC can suffer in the presence of severe class imbalance. Here’s a way to see how PR AUC might do better.</summary></entry><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/posts/welcome-to-jekyll" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2022-07-23T16:37:26-04:00</published><updated>2022-07-23T16:37:26-04:00</updated><id>http://localhost:4000/posts/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/posts/welcome-to-jekyll">&lt;p&gt;Math: $f(x) = 5$&lt;/p&gt;

&lt;p&gt;You’ll find this post in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;Jekyll requires blog post files to be named according to the following format:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YEAR-MONTH-DAY-title.MARKUP&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YEAR&lt;/code&gt; is a four-digit number, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MONTH&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DAY&lt;/code&gt; are both two-digit numbers, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MARKUP&lt;/code&gt; is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Tom&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &apos;Hi, Tom&apos; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">Math: $f(x) = 5$</summary></entry></feed>