{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7337d373",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "permalink: tensor-products-grads\n",
    "title: Understanding Shapes of Matrix Products and Gradients\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87d66add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from manim import *\n",
    "# use -r width,height to make it bigger or smaller. Use .scale to scale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e57e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -qm -r 640,480 -v WARNING SetColumnColorsExample\n",
    "\n",
    "# hide\n",
    "\n",
    "# class SquareToCircle(Scene):\n",
    "#    def construct(self):\n",
    "#       square = Square()\n",
    "#       circle = Circle()\n",
    "#       circle.set_fill(PINK, opacity=0.5)\n",
    "#       self.play(Create(square))\n",
    "#       self.play(Transform(square, circle))\n",
    "#       self.wait()\n",
    "class SetColumnColorsExample(Scene):\n",
    "    def construct(self):\n",
    "        m = MathTex(r\"\\frac{dy}{dx}\")\n",
    "        m0 = Matrix([[\"\\pi\", 1], [-1, 3]],\n",
    "        ).set_column_colors([RED], GREEN).scale(3)\n",
    "        self.add(m0)\n",
    "        self.add(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759de7d",
   "metadata": {},
   "source": [
    "Taking the derivative of a scalar with respect to a scalar is one thing, but what about a vector and a vector, or a matrix and a matrix? What are the dimensions then?\n",
    "\n",
    "In this post I hope to clarify some of these by applying this key concept over and over:\n",
    "\n",
    ">The derivative $\\frac{dy}{dx}$ tells you how much $y$ changes when you increase $x$ by a little bit.\n",
    "\n",
    "$x$ and $y$ could be anything here-- scalars, vectors, matrices or tensors.\n",
    "\n",
    "From this concept, we can already learn something about the shape of $\\frac{dy}{dx}$. It must somehow tell us, for each dimension of $y$, how much a small increase in each dimension of $x$ would change it by.\n",
    "\n",
    "## Scalar and a scalar\n",
    "First let's see a concrete example of how this works with scalars (numbers). Consider the following function:\n",
    "\n",
    "$$ y = x^2 $$\n",
    "\n",
    "We know the derivative of this is:\n",
    "\n",
    "$$ \\frac{dy}{dx} = 2x $$\n",
    "\n",
    "If we increase $x$ by a little bit, $y$ will increase by $2x$. Applying our concept above, $\\frac{dy}{dx}$ looks at each dimension of $y$ and tells us how much a small increase in $x$ would change it by. Since $y$ only has one dimension and $x$ only has one dimension, there's only one change to look at. So $\\frac{dy}{dx}$ also has one dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0efff",
   "metadata": {},
   "source": [
    "## Vector and a scalar\n",
    "Now let's consider a a vector $\\mathbf{y}$ and scalar $x$ like so:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "From now on I'll write values like $a$, $b$, $c$ instead of using actual functions of $x$, which should keep things a little cleaner. Just imagine each of $a$, $b$, $c$ somehow depends on $x$. For example, maybe $a = x^2$.\n",
    "\n",
    "Applying our concept again, $\\frac{d\\mathbf{y}}{dx}$ looks at each dimension of $\\mathbf{y}$ to see how much a small increase in $x$ would change it by. Since there are 3 dimensions in $\\mathbf{y}$ to look at and only one in $x$, we would expect $\\frac{d\\mathbf{y}}{dx}$ to have 3 dimensions. Each dimension tells us how much a small increase in $x$ will change that dimension of $\\mathbf{y}$ by.\n",
    "\n",
    "If we carry out this logic and do the math:\n",
    "$$\n",
    "\\frac{d\\mathbf{y}}{dx} = \\begin{bmatrix} \\frac{da}{dx} \\\\ \\frac{db}{dx} \\\\ \\frac{dc}{dx} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We see that $\\frac{d\\mathbf{y}}{dx}$ indeed has 3 dimensions as expected. The first one tells us how much the first dimension of $\\mathbf{y}$ changes by when we increase $x$ by a small amount. The second tells us how much the second dimension of $\\mathbf{y}$ changes by when we increase $x$ by a small amount. And same for the third."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b1552",
   "metadata": {},
   "source": [
    "## Scalar and a vector:\n",
    "Now let's flip the above! Let's make $y$ a scalar and $\\mathbf{x}$ a vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We want to know:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{d\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "We can apply the same concept again. $\\frac{dy}{d\\mathbf{x}}$ looks at each dimension of $y$ and tells us how much a small increase in $\\mathbf{x}$ will change it by. But this time $\\mathbf{x}$ has 3 dimensions and $y$ has only one dimension. So, $\\frac{dy}{d\\mathbf{x}}$ will tell us how much the one dimension of $y$ is changed by _each_ of the 3 dimensions of $\\mathbf{x}$. It will have 3 dimensions like so:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{d\\mathbf{x}}= \\begin{bmatrix} \\frac{dy}{da} & \\frac{dy}{db} & \\frac{dy}{dc} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As an aside, you may be wondering why I wrote the derivative vector horizontally, and how to know when it should be horizontal or vertical? I could have just as easily written this:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{d\\mathbf{x}}= \\begin{bmatrix} \\frac{dy}{da} \\\\ \\frac{dy}{db} \\\\ \\frac{dy}{dc} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are actually two [conventions](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions) around which way to do it, but not everyone is consistent. \n",
    "\n",
    "I've picked the one where each row corresponds to a dimension of the derivative's \"numerator\" $y$, and each column corresponds to a dimension of the derivative's \"denominator\" $\\mathbf{x}$. Since the numerator is a scalar and has only one dimension, there is only one row. And since the denominator has 3 dimensions, there are 3 columns. We'll see this again in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86b1ef",
   "metadata": {},
   "source": [
    "## Vector and a vector\n",
    "Now things are getting interesting! We'll make $\\mathbf{x}$ and $\\mathbf{y}$ both vectors now:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix} q \\\\ r \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And we want to know:\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathbf{y}}{d\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "We can again apply the same concept! We want to know how much each dimension of $\\mathbf{y}$ is changed by each dimension of $\\mathbf{x}$. \n",
    "\n",
    "$\\mathbf{y}$ has 2 dimensions, and for each of those 2 dimensions we need to check how each of the 3 dimensions of $\\mathbf{x}$ will change it by. So we expect $\\frac{d\\mathbf{y}}{d\\mathbf{x}}$ to have 2x3 = 6 different entries.\n",
    "They are usually arranged into a matrix like so (again each row will represent a dimension of the numerator, and each column will represent a dimension of the denominator):\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathbf{y}}{d\\mathbf{x}} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{dq}{da} & \\frac{dq}{db} & \\frac{dq}{dc} \\\\\n",
    "\\frac{dr}{da} & \\frac{dr}{db} & \\frac{dr}{dc} &\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "We'll describe the matrix's dimensionality as `2 x 3`, so it has 2 rows and 3 columns. One row for each dimension of $\\mathbf{y}$, and one column for each dimension of $\\mathbf{x}$.\n",
    "\n",
    "In other words, if we look at position `(i, j)` in this matrix (ex. `(2, 1)` corresponding to row 2 column 1, which contains $\\frac{dr}{da}$), it will tell us how much dimension `i` in $\\mathbf{y}$ is changed by a small increase in dimension `j` of $\\mathbf{x}$.\n",
    "\n",
    "Fun fact, this particular matrix that you get from $\\frac{d\\mathbf{y}}{d\\mathbf{x}}$ when $\\mathbf{y}$ and $\\mathbf{x}$ are both vectors has a special name: The [Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant).\n",
    "\n",
    "## Shifting Perspective\n",
    "Let's pause here and look at another way of understanding the dimensions of the above scenarios.\n",
    "* **Scalar by a scalar**: In this case $y$ and $x$ were both scalars (one dimension). So the dimension of the derivative was: $dim(y) * dim(x)$ = `1 x 1`\n",
    "* **Scalar by a vector**: In this case, $y$ was a scalar but $\\mathbf{x}$ was a 3 dimensional vector. So the dimension of the derivative was $dim(y) * dim(x)$ = `1 x 3`\n",
    "* **Vector by a vector**: In this case, $\\mathbf{y}$ was a 2 dimensional vector and $\\mathbf{x}$ was a 3 dimensional vector. So the dimension of the derivative was $dim(y) * dim(x)$ = `2 x 3`\n",
    "\n",
    "We just took the dimensions and stuck all of $y$'s dimensions first, followed by all of $x$'s. Moreover, when we have a shape like `2 x 3`, entry `(i, j)` tells us how much a small change in the `jth` dimension of $x$ will change the `i`th dimension of $y$ by.\n",
    "\n",
    "## Matrix by a vector\n",
    "Now we can tackle more complicated things using this perspective shift. Consider:\n",
    "* A `4 x 5` matrix $\\mathbf{Y}$\n",
    "* A 7 dimensional vector $\\mathbf{x}$.\n",
    "\n",
    "As usual, want to know:\n",
    "$$\n",
    "\\frac{d\\mathbf{Y}}{d\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "$\\frac{d\\mathbf{Y}}{d\\mathbf{x}}$ looks at each of the `4 * 5 = 20` values in $\\mathbf{Y}$ and tells us how each of the 7 values in $\\mathbf{x}$ would change it by. We can organize this in the same way we have been so far. But now instead of a 2D matrix like `2 x 3`, we will have a 3D _tensor_ with dimensions `4 x 5 x 7`. We can think of a tensor as just a 3D list.\n",
    "\n",
    "## Matrix by a matrix\n",
    "Let's finish up with a matrix-matrix derivative. Consider:\n",
    "* A `4 x 5` matrix $\\mathbf{Y}$\n",
    "* A `7 x 3` matrix $\\mathbf{X}$.\n",
    "\n",
    "As usual, want to know:\n",
    "$$\n",
    "\\frac{d\\mathbf{Y}}{d\\mathbf{X}}\n",
    "$$\n",
    "\n",
    "$\\frac{d\\mathbf{Y}}{d\\mathbf{X}}$ looks at each of the `4 * 5 = 20` values in $\\mathbf{Y}$ and tells us how each of the `7 * 3 = 21` values in $\\mathbf{X}$ would change it by. We can organize this in the same way we have been so far. Our final 4D tensor will have the dimensions `4 x 5 x 7 x 3`.\n",
    "\n",
    "\n",
    "## Bonus: Machine Learning/Forward Layer\n",
    "So now we should be well equipped to compute any matrix/vector derivatives and know their dimensionality. If you're a ML person, you might be reading this due to its relevance to optimization techniques like gradient descent. As a bonus, let's look at the dimensionalities involved in gradients for the forward layer of a neural network.\n",
    "\n",
    "In a forward layer (leaving out the activation and bias term for simplicity), our model is just:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X}\\mathbf{W}\n",
    "$$\n",
    "\n",
    "In $\\mathbf{X}$, each row represents one of `N` data points, each of which is `M` dimensional, so it is an `N x M` dimensional matrix. $\\mathbf{W}$ is a `M x D` matrix that maps `M` dimensional things to `D` dimensional things. $\\mathbf{Y}$ are the outputs from this layer, and since there are `N` data points, we will have `N` sets of `D` dimensional outputs and this vector will be `N x D` dimensional.\n",
    "\n",
    "Assume we also have a loss function: \n",
    "\n",
    "$$\n",
    "L = scalar(\\mathbf{Y})\n",
    "$$\n",
    "\n",
    "For the sake of simplicity, it doesn't matter what $scalar$ means. All we need to know is it turns $\\mathbf{Y}$ into a scalar that represents how incorrect our model's predictions are, which all loss functions do in one way or another.\n",
    "\n",
    "\n",
    "Now, to optimize our weight matrix $\\mathbf{W}$, we need to compute:\n",
    "$$\n",
    "\\frac{dL}{d\\mathbf{W}}\n",
    "$$\n",
    "\n",
    "From our analysis, we know the dimensionality of this. $L$ is a scalar, and $\\mathbf{W}$ is a `M x D` dimensional matrix. The result will tell us how the scalar $L$ changes with each dimension of $\\mathbf{W}$. Since $\\mathbf{W}$ is `M x D`, $\\frac{dL}{d\\mathbf{W}}$ will also be `M x D`.\n",
    "\n",
    "Usually this quantity is computed using the chain rule, so:\n",
    "$$\n",
    "\\frac{dL}{d\\mathbf{W}} = \\frac{dL}{d\\mathbf{Y}} \\frac{d\\mathbf{Y}}{d\\mathbf{W}}\n",
    "$$\n",
    "\n",
    "Looking at each component:\n",
    "* $\\frac{dL}{d\\mathbf{Y}}$ is `N x D` dimensional since $L$ is a scalar and $\\mathbf{Y}$ is `N x D`.\n",
    "* $\\frac{d\\mathbf{Y}}{d\\mathbf{W}}$ is `(N x D) x (M x D)` dimensional, since $\\mathbf{Y}$ is `N x D` and $\\mathbf{W}$ is `M x D`\n",
    "\n",
    "But hold on a second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd236e35",
   "metadata": {},
   "source": [
    "# hide\n",
    "In this matrix, position $(i, j)$ tells you how much the $i^{th}$ dimension of $\\mathbf{y}$ changes when you change the $j^{th}$ dimension of $\\mathbf{x}$ by a little bit. For example, position $(2, 1)$ (row 2 column 1), the one containing $\\frac{dr}{da}$, tells us how much the 2nd dimension of $\\mathbf{y}$ changes when we change the 1st dimension of $\\mathbf{x}$ by a little bit.\n",
    "\n",
    "\n",
    "\n",
    "We also have a loss function $L = \\sum (\\mathbf{y} - \\mathbf{t})^2$, where $\\mathbf{t}$ is an `N x 1` dimensional vector of target values, and $\\mathbf{y}$ are the predictions from our model. The $\\sum$ sums over all of the entries, so $L$ will just be a scalar.\n",
    "\n",
    "where $\\mathbf{W}$ is a `M x D` dimensional weight vector and $\\mathbf{X}$ is a `N x M` matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "493fc9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd4b9938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\nlayout: post\\npermalink: other-url\\n---'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=\"---\\n.*\\n---\"\n",
    "s = \"\"\"\n",
    "# Tensor Products and Gradients\n",
    "\n",
    "\n",
    "---\n",
    "layout: post\n",
    "permalink: other-url\n",
    "---\n",
    "This post. And another line.\n",
    "\n",
    "\"\"\"\n",
    "re.search(pattern, s, re.DOTALL).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43440350",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"2022-05-something\"\n",
    "re.search(\"[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38085d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's begin\n",
      "$$\n",
      "some \\\\\\ text \\\\\\\n",
      "$$\n",
      "and then \\\\\n",
      "$$\n",
      "hello \\\\\\ world\n",
      "$$\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern=r\"(\\$\\$.+?\\$\\$)\"\n",
    "s = r\"\"\"\n",
    "Now let's begin\n",
    "$$\n",
    "some \\\\ text \\\\\n",
    "$$\n",
    "and then \\\\\n",
    "$$\n",
    "hello \\\\ world\n",
    "$$\n",
    "ok\n",
    "\"\"\"\n",
    "for t in re.findall(pattern, s, re.DOTALL):\n",
    "    tt = t.replace(\"\\\\\\\\\", \"\\\\\\\\\\\\\")\n",
    "    s = s.replace(t, tt)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c887b826",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd73a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
